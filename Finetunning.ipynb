{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9b4967c0c9f840958f9ac2fe69ba8f64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_17c5ffb4f2ce46cf87b2abdfaed3a26c",
              "IPY_MODEL_4acada5e9216497880af3717a1032f1e",
              "IPY_MODEL_2fa19fc761e647b4870c4b356fb54283"
            ],
            "layout": "IPY_MODEL_dcd5554dcaf04fb69681ab0a50cc72ae"
          }
        },
        "17c5ffb4f2ce46cf87b2abdfaed3a26c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c479927f978242fa80e8a3d7c119cbe1",
            "placeholder": "​",
            "style": "IPY_MODEL_e13a9245094c46aeb30f04936905c5ca",
            "value": "Map: 100%"
          }
        },
        "4acada5e9216497880af3717a1032f1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_164a4bd2edb24ee6bdd5b130791c6119",
            "max": 29258,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_472ba5f990114293ba910ed5f55b4c4d",
            "value": 29258
          }
        },
        "2fa19fc761e647b4870c4b356fb54283": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c717873f347f467e9e7abe3c0a1da721",
            "placeholder": "​",
            "style": "IPY_MODEL_d0ed0fd25814496695aa254e59e30749",
            "value": " 29258/29258 [01:20&lt;00:00, 375.53 examples/s]"
          }
        },
        "dcd5554dcaf04fb69681ab0a50cc72ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c479927f978242fa80e8a3d7c119cbe1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e13a9245094c46aeb30f04936905c5ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "164a4bd2edb24ee6bdd5b130791c6119": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "472ba5f990114293ba910ed5f55b4c4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c717873f347f467e9e7abe3c0a1da721": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0ed0fd25814496695aa254e59e30749": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ViUFcQsq1fjM"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[torch] accelerate>=0.20.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m24lKgiR2j1z",
        "outputId": "9a3ffcc2-bc17-479a-fd79-db962b991a33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.17.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.14.5 dill-0.3.7 multiprocess-0.70.15 xxhash-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import datasets\n",
        "import torch\n",
        "import time\n",
        "import logging"
      ],
      "metadata": {
        "id": "wlT7VoYN1rj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForCausalLM\n",
        "from transformers import TrainingArguments\n",
        "from transformers import AutoModelForCausalLM\n",
        "from datasets import Dataset\n",
        "logger = logging.getLogger(__name__)\n",
        "global_config = None"
      ],
      "metadata": {
        "id": "6mqJ9WBE15sf"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"EleutherAI/pythia-70m\"\n",
        "dataset_path = \"lamini/lamini_docs\"\n",
        "dataset_path = \"yahoo_answers_qa\"\n",
        "dataset_path = \"covid_qa_deepset\"\n",
        "dataset_path = \"wiki_qa\""
      ],
      "metadata": {
        "id": "5-9OT-o82Qub"
      },
      "execution_count": 266,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "5M7kASth2bAk"
      },
      "execution_count": 317,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.load_dataset(dataset_path)"
      ],
      "metadata": {
        "id": "BqNm7Q_i20v7"
      },
      "execution_count": 318,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions = dataset[\"train\"][\"question\"] + dataset[\"test\"][\"question\"] + dataset[\"validation\"][\"question\"]\n",
        "answers = dataset[\"train\"][\"answer\"] + dataset[\"test\"][\"answer\"] + dataset[\"validation\"][\"answer\"]"
      ],
      "metadata": {
        "id": "zeyrd8qNEAMd"
      },
      "execution_count": 322,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# array_of_dicts = []\n",
        "# for j in dataset[\"train\"]:\n",
        "#   question = j[\"question\"]\n",
        "#   for i in j['nbestanswers']:\n",
        "#     new_dict = {\"question\": question, \"answer\": i}\n",
        "#     array_of_dicts.append(new_dict)\n",
        "#   break\n",
        "\n",
        "# data_dict = {key: [d[key] for d in array_of_dicts] for key in array_of_dicts[0]}\n",
        "# my_dataset = Dataset.from_dict(data_dict)"
      ],
      "metadata": {
        "id": "Qx94UZHwc-DZ"
      },
      "execution_count": 271,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# questions = []\n",
        "# answers = []\n",
        "# k = 0\n",
        "# for j in dataset[\"train\"]:\n",
        "#   if k == 10000:\n",
        "#     break\n",
        "#   question = j[\"question\"]\n",
        "#   for i in j['nbestanswers']:\n",
        "#     questions.append(j[\"question\"])\n",
        "#     answers.append(i)\n",
        "#   k = k + 1"
      ],
      "metadata": {
        "id": "_z7SEO4-oBGb"
      },
      "execution_count": 272,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dict = {\n",
        "    'question': questions,\n",
        "    'answer': answers\n",
        "}\n",
        "fine_tunning_dataset = Dataset.from_dict(data_dict)"
      ],
      "metadata": {
        "id": "UAs-2VJlwJP3"
      },
      "execution_count": 329,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    if \"question\" in examples and \"answer\" in examples:\n",
        "      text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
        "    elif \"input\" in examples and \"output\" in examples:\n",
        "      text = examples[\"input\"][0] + examples[\"output\"][0]\n",
        "    else:\n",
        "      text = examples[\"text\"][0]\n",
        "\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenized_inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"np\",\n",
        "        padding=True,\n",
        "    )\n",
        "\n",
        "    max_length = min(\n",
        "        tokenized_inputs[\"input_ids\"].shape[1],\n",
        "        2048\n",
        "    )\n",
        "    tokenizer.truncation_side = \"left\"\n",
        "    tokenized_inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"np\",\n",
        "        truncation=True,\n",
        "        max_length=max_length\n",
        "    )\n",
        "\n",
        "    return tokenized_inputs"
      ],
      "metadata": {
        "id": "gPaO9E5AqF0r"
      },
      "execution_count": 331,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = fine_tunning_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    batch_size=1,\n",
        "    drop_last_batch=True\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "9b4967c0c9f840958f9ac2fe69ba8f64",
            "17c5ffb4f2ce46cf87b2abdfaed3a26c",
            "4acada5e9216497880af3717a1032f1e",
            "2fa19fc761e647b4870c4b356fb54283",
            "dcd5554dcaf04fb69681ab0a50cc72ae",
            "c479927f978242fa80e8a3d7c119cbe1",
            "e13a9245094c46aeb30f04936905c5ca",
            "164a4bd2edb24ee6bdd5b130791c6119",
            "472ba5f990114293ba910ed5f55b4c4d",
            "c717873f347f467e9e7abe3c0a1da721",
            "d0ed0fd25814496695aa254e59e30749"
          ]
        },
        "id": "kZt_FI1Zr51-",
        "outputId": "cd11038b-78a0-492e-aef1-2a4db8d1da35"
      },
      "execution_count": 332,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/29258 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9b4967c0c9f840958f9ac2fe69ba8f64"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = tokenized_dataset.add_column(\"labels\", tokenized_dataset[\"input_ids\"])"
      ],
      "metadata": {
        "id": "BwbzYMZKswc-"
      },
      "execution_count": 333,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, shuffle=True, seed=123)"
      ],
      "metadata": {
        "id": "JXzTa3CNtzSj"
      },
      "execution_count": 334,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = split_dataset[\"train\"]\n",
        "test_dataset = split_dataset[\"test\"]"
      ],
      "metadata": {
        "id": "eG6I7iAF24V2"
      },
      "execution_count": 335,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "5aA_jC223J0Y"
      },
      "execution_count": 336,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checks if there is a availability of the GPU and selects it\n",
        "device_count = torch.cuda.device_count()\n",
        "if device_count > 0:\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "2O3PpQph3OK0"
      },
      "execution_count": 337,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLD3S0yV3Sdn",
        "outputId": "beb85b4f-f999-4686-9ce8-7253cddf3748"
      },
      "execution_count": 338,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTNeoXForCausalLM(\n",
              "  (gpt_neox): GPTNeoXModel(\n",
              "    (embed_in): Embedding(50304, 512)\n",
              "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
              "    (layers): ModuleList(\n",
              "      (0-5): 6 x GPTNeoXLayer(\n",
              "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (attention): GPTNeoXAttention(\n",
              "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
              "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
              "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (mlp): GPTNeoXMLP(\n",
              "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (act): GELUActivation()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 338
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#encodes, Generates, and Decodes\n",
        "def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n",
        "  # Tokenize\n",
        "  input_ids = tokenizer.encode(\n",
        "          text,\n",
        "          return_tensors=\"pt\",\n",
        "          truncation=True,\n",
        "          max_length=max_input_tokens\n",
        "  )\n",
        "\n",
        "  # Generate\n",
        "  device = model.device\n",
        "  generated_tokens_with_prompt = model.generate(\n",
        "    input_ids=input_ids.to(device),\n",
        "    max_length=max_output_tokens\n",
        "  )\n",
        "\n",
        "  # Decode\n",
        "  generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
        "\n",
        "  # Strip the prompt\n",
        "  generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
        "\n",
        "  return generated_text_answer"
      ],
      "metadata": {
        "id": "zMqhFdVf3V0R"
      },
      "execution_count": 339,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_text = test_dataset[50]['question']\n",
        "print(\"Question input (test):\", test_text)\n",
        "print(f\"Correct answer: {test_dataset[0]['answer']}\")\n",
        "print(\"Model's answer: \")\n",
        "print(inference(test_text, base_model, tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkDv6MKS3cZI",
        "outputId": "70557b9b-6673-460b-b9ca-c10e30e373dd"
      },
      "execution_count": 373,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question input (test): What is the first program to run during a system boot sequence?\n",
            "Correct answer: This increased attention has also highlighted labor and human rights issues concerning the city's largely South Asian paid workforce.\n",
            "Model's answer: \n",
            "The first program is a program that runs on a computer running on a computer running on a computer running on a computer running on a computer running on a computer running on a computer running on a computer running on a computer running on a computer running on a computer running on a computer running on a computer running on a computer running on a computer running on a computer running on a computer running on a computer running on a computer running on a computer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_steps = 50"
      ],
      "metadata": {
        "id": "cX3fuu4I3jVC"
      },
      "execution_count": 341,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trained_model_name = f\"lamini_docs_{max_steps}_steps\"\n",
        "# output_dir = trained_model_name"
      ],
      "metadata": {
        "id": "1SrP_I_m3mQf"
      },
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trained_model_name = f\"yahoo_docs_{max_steps}_steps\"\n",
        "# output_dir = trained_model_name"
      ],
      "metadata": {
        "id": "OZSGQqZ13sjZ"
      },
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model_name = f\"{dataset_path}_{max_steps}_steps\"\n",
        "output_dir = trained_model_name"
      ],
      "metadata": {
        "id": "4u9svIHSHeHZ"
      },
      "execution_count": 342,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "\n",
        "  # Learning rate\n",
        "  learning_rate=1.0e-5,\n",
        "\n",
        "  # Number of training epochs\n",
        "  num_train_epochs=1,\n",
        "\n",
        "  # Max steps to train for (each step is a batch of data)\n",
        "  # Overrides num_train_epochs, if not -1\n",
        "  max_steps=max_steps,\n",
        "\n",
        "  # Batch size for training\n",
        "  per_device_train_batch_size=1,\n",
        "\n",
        "  # Directory to save model checkpoints\n",
        "  output_dir=output_dir,\n",
        "\n",
        "  # Other arguments\n",
        "  overwrite_output_dir=False, # Overwrite the content of the output directory\n",
        "  disable_tqdm=False, # Disable progress bars\n",
        "  eval_steps=120, # Number of update steps between two evaluations\n",
        "  save_steps=120, # After # steps model is saved\n",
        "  warmup_steps=1, # Number of warmup steps for learning rate scheduler\n",
        "  per_device_eval_batch_size=1, # Batch size for evaluation\n",
        "  evaluation_strategy=\"steps\",\n",
        "  logging_strategy=\"steps\",\n",
        "  logging_steps=1,\n",
        "  optim=\"adafactor\",\n",
        "  gradient_accumulation_steps = 4,\n",
        "  gradient_checkpointing=False,\n",
        "\n",
        "  # Parameters for early stopping\n",
        "  load_best_model_at_end=True,\n",
        "  save_total_limit=1,\n",
        "  metric_for_best_model=\"eval_loss\",\n",
        "  greater_is_better=False\n",
        ")"
      ],
      "metadata": {
        "id": "vy9rI6Yc10Bd"
      },
      "execution_count": 343,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_flops = (\n",
        "  base_model.floating_point_ops(\n",
        "    {\n",
        "       \"input_ids\": torch.zeros(\n",
        "           (1, 2048)\n",
        "      )\n",
        "    }\n",
        "  )\n",
        "  * training_args.gradient_accumulation_steps\n",
        ")\n",
        "\n",
        "print(base_model)\n",
        "print(\"Memory footprint\", base_model.get_memory_footprint() / 1e9, \"GB\")\n",
        "print(\"Flops\", model_flops / 1e9, \"GFLOPs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xwrc_HkT3tt7",
        "outputId": "b62947bb-8a16-4014-f139-c2794fa43d40"
      },
      "execution_count": 344,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPTNeoXForCausalLM(\n",
            "  (gpt_neox): GPTNeoXModel(\n",
            "    (embed_in): Embedding(50304, 512)\n",
            "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x GPTNeoXLayer(\n",
            "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
            "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
            "        (attention): GPTNeoXAttention(\n",
            "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
            "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
            "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (mlp): GPTNeoXMLP(\n",
            "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (act): GELUActivation()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
            ")\n",
            "Memory footprint 0.306872536 GB\n",
            "Flops 2195.667812352 GFLOPs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Trainer class to include logging and history\n",
        "class Trainer(transformers.Trainer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        model_flops,\n",
        "        total_steps,\n",
        "        args=None,\n",
        "        data_collator=None,\n",
        "        train_dataset=None,\n",
        "        eval_dataset=None,\n",
        "        tokenizer=None,\n",
        "        model_init=None,\n",
        "        compute_metrics=None,\n",
        "        callbacks=None,\n",
        "        optimizers=(None, None),\n",
        "    ):\n",
        "        super(Trainer, self).__init__(\n",
        "            model,\n",
        "            args,\n",
        "            data_collator,\n",
        "            train_dataset,\n",
        "            eval_dataset,\n",
        "            tokenizer,\n",
        "            model_init,\n",
        "            compute_metrics,\n",
        "            callbacks,\n",
        "            optimizers,\n",
        "        )\n",
        "\n",
        "        self.total_steps = total_steps\n",
        "        self.model_flops = model_flops\n",
        "        self.start_step = 0\n",
        "\n",
        "    def training_step(self, model, inputs):\n",
        "        if inputs[\"input_ids\"].numel() == 0:\n",
        "\n",
        "          print(\"Inputs: \", inputs)\n",
        "          print(\"Inputs - input_ids\", inputs[\"input_ids\"])\n",
        "          print(\"numel\", inputs[\"input_ids\"].numel())\n",
        "\n",
        "          return torch.tensor(0)\n",
        "        else:\n",
        "          model.train()\n",
        "          inputs = self._prepare_inputs(inputs)\n",
        "\n",
        "          with self.compute_loss_context_manager():\n",
        "              loss = self.compute_loss(model, inputs)\n",
        "\n",
        "          if self.args.n_gpu > 1:\n",
        "              loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
        "\n",
        "          if self.do_grad_scaling:\n",
        "              self.scaler.scale(loss).backward()\n",
        "          else:\n",
        "              self.accelerator.backward(loss)\n",
        "\n",
        "          return loss.detach() / self.args.gradient_accumulation_steps\n",
        "\n",
        "    def log(self, logs):\n",
        "        \"\"\"\n",
        "        Log `logs` on the various objects watching training.\n",
        "        Subclass and override this method to inject custom behavior.\n",
        "        Args:\n",
        "            logs (`Dict[str, float]`):\n",
        "                The values to log.\n",
        "        \"\"\"\n",
        "        if self.state.epoch is not None:\n",
        "            logs[\"epoch\"] = round(self.state.epoch, 2)\n",
        "\n",
        "        self.update_log_timing(logs)\n",
        "\n",
        "        output = {**logs, **{\"step\": self.state.global_step}}\n",
        "        self.update_history(output)\n",
        "\n",
        "        logger.debug(\"Step (\" + str(self.state.global_step) + \") Logs: \" + str(logs))\n",
        "        self.control = self.callback_handler.on_log(\n",
        "            self.args, self.state, self.control, logs\n",
        "        )\n",
        "\n",
        "    def update_log_timing(self, logs):\n",
        "        if len(self.state.log_history) == 0:\n",
        "            self.start_time = time.time()\n",
        "            logs[\"iter_time\"] = 0.0\n",
        "            logs[\"flops\"] = 0.0\n",
        "            logs[\"remaining_time\"] = 0.0\n",
        "            self.start_step = self.state.global_step\n",
        "        elif self.state.global_step > self.start_step:\n",
        "            logs[\"iter_time\"] = (time.time() - self.start_time) / (\n",
        "                self.state.global_step - self.start_step\n",
        "            )\n",
        "            logs[\"flops\"] = self.model_flops / logs[\"iter_time\"]\n",
        "            logs[\"remaining_time\"] = (self.total_steps - self.state.global_step) * logs[\n",
        "                \"iter_time\"\n",
        "            ]\n",
        "\n",
        "    def update_history(self, output):\n",
        "        if \"eval_loss\" in output:\n",
        "            return\n",
        "        if len(self.state.log_history) > 0:\n",
        "            smoothing_window = 100\n",
        "            p = 1.0 / smoothing_window\n",
        "            if \"loss\" in output:\n",
        "                output[\"loss\"] = output[\"loss\"] * p + self.state.log_history[-1][\n",
        "                    \"loss\"\n",
        "                ] * (1.0 - p)\n",
        "        self.state.log_history.append(output)"
      ],
      "metadata": {
        "id": "9OdVSO2Q-eYR"
      },
      "execution_count": 345,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=base_model,\n",
        "    model_flops=model_flops,\n",
        "    total_steps=max_steps,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")"
      ],
      "metadata": {
        "id": "HyeviU504Cft"
      },
      "execution_count": 346,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_output = trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "vmcWh7dv5Kc7",
        "outputId": "554511d2-f5ef-42c7-ee54-59c92ba23905"
      },
      "execution_count": 347,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 02:18, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_dir = f'{output_dir}/final'\n",
        "\n",
        "trainer.save_model(save_dir)\n",
        "print(\"Saved model to:\", save_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmINq_be6OwY",
        "outputId": "ea7bf1e6-8808-4e9e-8b66-50fa298c7e12"
      },
      "execution_count": 350,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model to: wiki_qa_50_steps/final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "finetuned_slightly_model = AutoModelForCausalLM.from_pretrained(save_dir, local_files_only=True)"
      ],
      "metadata": {
        "id": "Q0GrBHKS6Tv1"
      },
      "execution_count": 351,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetuned_slightly_model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7i4OoIzk6Y-7",
        "outputId": "7b25e542-666e-442f-9c99-8ab0c3955ca1"
      },
      "execution_count": 352,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTNeoXForCausalLM(\n",
              "  (gpt_neox): GPTNeoXModel(\n",
              "    (embed_in): Embedding(50304, 512)\n",
              "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
              "    (layers): ModuleList(\n",
              "      (0-5): 6 x GPTNeoXLayer(\n",
              "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (attention): GPTNeoXAttention(\n",
              "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
              "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
              "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (mlp): GPTNeoXMLP(\n",
              "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (act): GELUActivation()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 352
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_question = test_dataset[50]['question']\n",
        "print(\"Question input (test):\", test_question)\n",
        "\n",
        "print(\"Finetuned slightly model's answer: \")\n",
        "print(inference(test_question, finetuned_slightly_model, tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlWnFHR_2xIy",
        "outputId": "19b3153a-2a06-4e7b-94e6-92d7c063ebc1"
      },
      "execution_count": 372,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question input (test): What is the first program to run during a system boot sequence?\n",
            "Finetuned slightly model's answer: \n",
            "The first program is a program that runs on a computer running on a computer running on a computer running on a computer running on a computer running on a computer running on a computer running on a computer running on a computer running on a computer running on a computer running on a computer running on a computer running on a computer running on a computer running on a computer running on a computer running on a computer running on a computer running on a computer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_question = test_dataset[24]['question']\n",
        "print(\"Question input (test):\", test_question)\n",
        "# test_answer = test_dataset[24]['answer']\n",
        "# print(\"Target answer output (test):\", test_answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWtYcgOK0vav",
        "outputId": "05e85afd-bd64-4767-bbf8-6ab9c750d543"
      },
      "execution_count": 368,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question input (test): what happened to the new condos built at st clair ave and mt vernon in columbus ohio\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_question = test_dataset[24]['question']\n",
        "print(\"Question input (test):\", test_question)\n",
        "# test_answer = test_dataset[24]['answer']\n",
        "# print(\"Target answer output (test):\", test_answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AG-tlDROONwh",
        "outputId": "baa59b6c-29c1-47ac-a57c-dea417b7f9ff"
      },
      "execution_count": 371,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question input (test): what happened to the new condos built at st clair ave and mt vernon in columbus ohio\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a-TYH2_X2u3X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}